<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>WHOLE EARTH CODEC</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="banner">
      <div id="title">WHOLE EARTH CODEC</div>
    </div>
  </header>
  <main>
      <div id="titleSection" class="visible">
          <img id="points" src="img/pointcloud_full.png"/>
        <div class="titleText">WHOLE<br />EARTH<br />CODEC
        </div>
      </div>
      <div id="toc">
        <div class="contents">
          <a href="#act1"><h1>
            Act 1: Mesh Observatory
          </h1>
          <p>
            Inverting the gaze, synthesizing data streams.
          </p></a>
        </div>
        <div class="contents">
          <a href="#act2"><h1>
            Act 2: Biosphere and Technosphere
          </h1>
          <p>
            Digitizing Earth’s Umwelt.
          </p></a>
        </div>
        <div class="contents">
          <a href="#act3"><h1>
            Act 3: New Planet Sensorium
          </h1>
          <p>
            Amalgamated and augmented landscape.
          </p></a>
        </div>
      </div>
      <div id="act1" class="textSection">
        <h1>
          Act 1:<br />Mesh<br />Observatory
        </h1>
        <div id="pointsDisappear" class="text">
          <p>“can you explain foundation models”</p>

<p>
  <i>Sure, I'd be happy to explain foundation models! Foundation models refer to large-scale language models that are trained on massive amounts of text data using techniques like unsupervised learning, which allows them to understand and generate natural language at a very sophisticated level. These models form the basis of many modern natural language processing (NLP) applications and are used to perform tasks like language translation, text classification, question answering, and much more.
</i>
</p>
<p>
Foundation models typically consist of a large number of layers of artificial neurons organized in a complex architecture, with each layer responsible for processing a specific aspect of the input text. The parameters of these models are learned through a process called pre-training, which involves training the model on a large corpus of text data, often consisting of billions of words or more.
Once a foundation model has been pre-trained, it can be fine-tuned for specific NLP tasks by training it on a smaller dataset that is specific to the task at hand. Fine-tuning allows the model to adapt to the nuances of the target task, and can result in highly accurate and effective models.
</p><p>
Generated using ChatGPT, the above excerpt simultaneously expresses both the power and limitations of foundation models, as currently conceived. Certainly, this model was able to generate a more concise and legible response than any of the authors of this paper could produce in a similar time. Despite this “very sophisticated” response, however, the above dialogue also reveals one of the central limitations of the current generation of foundation models. In claiming that foundation models are “large-scale language models” used to “generate natural language”, ChatGPT mistakes its own particularity for a generality. While most foundation models have been trained on text scraped from the web, there is nothing that limits these models to dealing with text alone. The true power of foundation models rests in their multi-modal ability to combine many different streams of information (text, image, sound, etc...) into one large model.
</p>
<div class="sticky">
  <img id="dataDiagram" src="img/Diagram1_v1.png"/>
</div>
<p id="data1">
ChatGPT’s underlying model, GPT-3, is what is known as a Large Language Model. Despite its impressive scale (GPT-3 has 175 billion parameters) such a model represents only a very small subset of data. The model is trained primarily using web data scraped from Common Crawl, a small and unrepresentative data set representing the “detritus” of human speech (reddit forums and other publicly accessible forms of data). The much broader set of potential data remains inaccessible due to privacy concerns, incompatible format, or lack of proper aggregation. [dalenaxtran@gmail.com insert something about data lakes]. If we are to construct large models that serve as the foundation for a wide range of future tasks, it is in the best interest of the model for this foundation to be as representative as possible of the rich diversity of information that gives structure to the world.
</p>
<p id="data2">
This proposal seeks to radically expand the scope of current foundation models, moving beyond anthropocentric, natural language-oriented data towards the wealth of non-anthropocentric information streams immanent to the biosphere. Should there be any hope that these foundation models might play a central role in the prevention of climate collapse and ecological catastrophe, we must better integrate the biosphere and technosphere.
</p>
<p>
Currently, many actors are attempting to gather such information, albeit in piecemeal and siloed ways. The Global Nucleic Acid Observatory and the Australian Acoustic Observatory, for example, seek to capture metagenomic information of watersheds and ecological bioacoustic sound data, for example. Both approaches invert the traditional model of the observatory, looking inwards towards earth systems rather than outwards into the cosmos. Still, both approaches remain relatively limited in scope and express no ambitions towards integrating the data they collect in a large, self-supervised foundation model. Another observatory provides another model for thinking through how a planetary-scale observatory might be conceived of. The Event Horizon Telescope is a global network of synchronized radio observatories that famously worked together to produce the first ever “image” of a black hole. Such a model is significant in that it recasts the entire earth as one distributed observatory, with each individual observatory contributing a small portion to a larger synthetic image of the cosmos. But what if such a distributed network were to be inverted in on itself? How might we imagine a distributed observatory that takes the earth itself as the object to be observed? And what if the cognitive infrastructure of such observations took the form of a foundation model, a true foundation that represented not only a small subset of human language but the wealth of information in the biosphere as well?

          </p>

        </div>
      </div>
      <div id="act2" class="textSection">
        <h1>
          Act 2:<br />Biosphere and<br />Technosphere
        </h1>
        <div class="text">
          <p>The Event Horizon Telescope produced the first image of a black hole by stitching together distributed measurements to output an inferred reconstruction. This calculation is no different to how we see: photoreceptors absorb electromagnetic radiation and trigger electrical signals traveling along neurons, beyond which higher levels of processing assemble the complete image. The transformation from one signal to another is called transduction, which describes both the process by which we see, hear, smell—but also what machine sensors do. We are all plugged into a limited bandwidth of the planetary ground truth of various forms of radiation, vibration, and energy, yet integrate the stimuli into a cohesive umwelt. The biosphere and technosphere must be synchronized; enter the Whole Earth Codec, an autoregressive foundation model trained across myriad planetary sensing modalities.
</p>
<div id="transduction">
  <img src="img/Diagram4_v1.png"/>
</div>
<p>
Data<br />
The Whole Earth Codec ingests broad spectrum data across modalities and is thus capable of synthesizing a wider understanding of the planet than any singular organism. It enables interoperability between disparate forms of data and allows an expansive planetary intelligence to emerge. The distributed network of the mesh observatory consists of different sensors receiving different types of stimuli: image, audio, chemical, lidar, pressure, moisture, magnetic fields, etc. What forms of data is produced is just as broad as the forms of sensing. The data stream from an individual sensor consists of measurements taken at a modality-relevant sampling rate, e.g. twenty times a second for earthquake seismometers, once every thirty minutes for common AQI sensors. The rate can also differ between sensors within the same modality, such as a microphone attuned to birdsong at 44 kHz versus one for turtles at 24 kHz. Regardless of modality, however, a UTC timestamp and GPS satellite signal is attached to each sample. This anchoring allows the model to make associations based on temporal and spatial correlation across disparate modes.
</p>
<div id="codec" class="sticky">
  <img src="img/Diagram5_v1.png"/>
</div>
<p>
Tokenization<br />
In data streams of sequential samples, the tokenization method is apparent: the data from these sensors are batched into an aggregated document and each sample from its stream of measurements is a single token. For example, a a token for a thermometer would be a single temperature reading. Other forms of non-sequential data require different tokenization techniques, such as images broken into pixels and sequenced in raster order.
</p><p>
Pre-training<br />
Foundation models are pre-trained on massive corpus of unsupervised data and the Whole Earth Codec is no different. To handle multimodal input, separate encoders are trained for each type of data. These encoders transform disparate forms of input into dense, high-dimensional embeddings within a single, massive cross-modal latent space. In order to do this, the model is trained to project temporally and spatially correlated forms of data into nearby embeddings within the space. Decoders of different modalities are then trained by translating the latent space embeddings into predicted sequences. Due to the massive size of data available, the model only makes a single pass over the continuous streams of data. This also means that as new data is gathered and aggregated over time, the model can simply continue training and updating weights.
</p><p>
Privacy<br />
The distributed sensing network from which the Whole Earth Codec observes the planet is privy to vasts amounts of data, yet sensitive information is protected through structured transparency techniques including federated learning and differential privacy. Input privacy refers to the ability to process information that is hidden from you and to allow others to process your information without revealing it to them, while output privacy refers to the ability to read the output of an information flow without being able to reverse-engineer further information about the input. Through federated learning, data from the mesh observatory is processed in local servers within a secure enclave, communicating weights rather than data to the coordinating server containing the main foundation model. This maintains the input privacy of all data ingested by the model. For particularly sensitive types of data, adding noise to every datapoint preserves output privacy without adverse effect to learned predictions overall; this technique is known as differential privacy. The sum of privacy-enhacing technologies deployed entails a trustless paradigm for training the Whole Earth Codec. Mutually assured observation between entities complicates traditional relations of opacity/transparency and antagonism/mutualism.
</p><p>
Capabilities<br />
From ingesting myriad channels of sensing data and integrating them into a shared latent space, the Whole Earth Codec makes emergent associations of the planetary. It operates between multiple temporal and spatial scales. Beyond the umwelt of any single organism, its sensorium is privy to an amalgamated landscape that melds technosphere and biosphere. Through abrupt specific capability scaling, the foundation model sees sharp increases in inference capability as the size of inputs increases; this motivates the Whole Earth Codec as a planetary project rather than a nation-state one.
<br />
Springing from the Codec is a rich ecosystem of third-party, fine-tuned models for improved performance on particular downstream tasks. Leveraging the pre-trained baseline, fine-tuning involves using a smaller, labeled dataset to update model weights, often for specific capabilities or to address domain shift. Within the ecosystem, there are fine-tuned models developed by an economy of research universities and private startups, available open-source or through pay-to-play APIs. Openly available models proliferate in everyday use among eco-hobbyists, but industries such as insurance pay a premium for high-performance proprietary software.
</p><p>
Governance<br />
Sovereignty is derived from this technology, but its implementation spreads from the bottom up. The mesh observatory is a conglomerate of public and private sensors, networked in by entities ranging from government institutions to research universities to individual hobbyists. Much like the Internet Engineering Task Force (or IETF), the standards and protocols of the Whole Earth Codec is maintained by a supra-national body and proposed, developed, and reviewed in an open process. This body maintains interoperable protocols for training and deploying the foundation model, governing processes including data transmission, federation, and weight aggregation.
</p><p>
          </p>
        </div>
      </div>
      <div id="act3" class="textSection">
        <h1>
          Act 3:<br />New Planet<br />Sensorium
        </h1>
        <div class="text">
        <p>
          One of the key features of foundation models is emergence; the self-supervised model and extremely large scale of the training data makes it hard to predict exactly what the downstream uses might be. This emergent nature challenges the traditional observatory model, in which scientists conduct observations in order to answer specific scientific hypotheses. What is an observatory wherein one does not know exactly what they are looking for? Through the assembly of the foundation model, secondary and tertiary modes of observation might emerge, where a metagenomic sequence might tell one about land use patterns. Observations are produced, rather than conducted. In line with this, this final section consists of a series of speculative case studies. While the nature of foundation models makes it incredibly difficult to predict their downstream usages, such speculation is crucial to understanding the potential benefits and risks that such models pose on the world, and how they may unseat long-held assumptions concerning sovereignty, surveillance, and global coordination.
        </p>
      </div>
        <div id="clouds">
          <video autoplay muted loop>
            <source src="https://users.dma.ucla.edu/~dalena/WEC/clouds.webm" type="video/mp4">
          </video></div>
          <div class="text">
            <p>
            Cloud seeding<br />
            Mongolia, a country still dependent on the burning of unrefined coal to heat homes, has some of the worst air pollution in the world. To remain under internationally mandated pollution controls, the environmental agency experiments with cloud seeding in an effort to use rain to clear out air particles. The strategy is effective, but neighboring China catches wind of the program and accuses Mongolia of “stealing their rainfall.” China then develops a fine-tuned model for detecting cloud seeding. This model combines object detection of satellite imagery with chemical analysis of the air in order to detect the cloud seeding efforts, and is quite effective at doing so.
            <br />
            The model is open-sourced and picked up by a small team of researchers in Somalia. In playing with the model, they discover that cloud seeding is also happening in the drought-stricken region by rogue agricultural producers looking to increase crop yield. This leads to a proliferation of sensors in the region, and an ecology of fine-tuned models suited to similar tasks.
            </p>
          </div>
        <div id="water">
          <video autoplay muted loop>
            <source src="https://users.dma.ucla.edu/~dalena/WEC/forest.webm" type="video/mp4">
          </video>
        </div>
        <div class="text">
          <p>
          Forest blight<br />
          Canada has been plagued by a pine beetle infestation and corresponding fungal disease. This has a large impact on the country’s timber industry, as the infestation kills chestnut trees before they reach maturity. In order to mitigate risk, insurance companies implement a proprietary sensing network in order to better predict the spread of the beetle. This information is kept from landowners, however, who see large price increases in their insurance premiums in areas where the beetle is predicted to spread. Insurance companies have a monopoly of information and are reluctant to prevent the spread of the beetle as corresponding premium hikes become a large source of profit.
          <br />
          Landowners learn that acoustic monitoring, combined with other remote sensing technologies, can more accurately predict the spread of the beetle. Pooling their individual data, they fine-tune a better model for early beetle detection, forming a decentralized “data union”. Information, rather than labor power, becomes the leverage point. High-resolution predictions enable targeted eradication of the beetle infestation, improving regional forest health. To encourage participation in the network, differential access to the fine-tuned model is granted based on the degree of participation in the scheme. For access, one must install sensors to increase the fidelity of the network, promoting mutual collaboration.
          </p>
        </div>
      <div class="text">
        <p>
          Gene drive detection<br />
CRISPR-Cas9 technologies have made the genetic engineering of ecosystems a technical possibility. Through a technique called gene drive, a particular suite of genes can be propagated through a population, permanently changing the genome of an entire species. There has been an international moratorium on gene drive implementation, but this has proven notoriously hard to regulate. To combat this, the EU implements a large-scale nucleic acid observatory. The observatory makes use of a mesh network of water sensors, which are used to conduct metagenomic sequencing of the surrounding watershed. The devices make use of federated learning technology to protect the sensitive genomic information from being collected, sharing only the model weights to the foundation model.
<br />
Meanwhile, in the UK there is popular resistance to what is perceived as a mass surveillance system. Under pressure, politicians reject the scheme in favor of a homegrown system. In the EU, the program proves remarkably efficient in preventing new gene drive efforts. The baseline metagenome of non-modified species is incorporated into the model, and any modifications to this baseline are immediately detected. Meanwhile, in the UK the system lacks the scale necessary to properly sequence the data, leaving the country vulnerable to gene drive attacks.
<br />
An ecoterrorist group exploits this vulnerability; engineering a gene drive that modifies the genes of livestock in order to render their meat tough and inedible, without harming the animals themselves. This strategy proves remarkably effective in reducing meat consumption (and related emissions) in the UK. Despite this, most livestock are killed, farmers are bailed out by the government, and the UK joins in the nucleic acid observatory in the end to prevent similar events from happening in the future.
        </p>
</div>
        <div id="water">
          <video autoplay muted loop>
            <source src="https://users.dma.ucla.edu/~dalena/WEC/water.webm" type="video/mp4">
          </video>
        </div>
      </div>
      </div>
  </main>
</body>
<script src="js/scroll.js"></script>
</html>
