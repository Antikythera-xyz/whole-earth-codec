<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>WHOLE EARTH CODEC</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="banner">
      <div id="title">WHOLE EARTH CODEC</div>
    </div>
  </header>
  <main>
      <div id="titleSection" class="visible">
          <video id="points" autoplay muted loop>
            <source src="https://users.dma.ucla.edu/~dalena/WEC/foundation.webm" type="video/mp4">
          </video>
        <div class="titleText">WHOLE<br />EARTH<br />CODEC
        </div>
      </div>
      <div id="toc">
        <div class="contents">
          <a href="#act1"><h1>
            Act 1: Inverting the Observatory
          </h1>
          <p>
            Gazing at Earth through distributed sensing.
          </p></a>
        </div>
        <div class="contents">
          <a href="#act2"><h1>
            Act 2: The<br />Whole Earth Codec
          </h1>
          <p>
            A foundation model for synthesizing biosphere and technosphere.
          </p></a>
        </div>
        <div class="contents">
          <a href="#act3"><h1>
            Act 3: New Planet Sensorium
          </h1>
          <p>
            On the emergent potentialities of amalgamated landscapes.
          </p></a>
        </div>
      </div>
      <div id="act1" class="textSection">
        <h1>
          Act 1:<br />Inverting the<br />Observatory
        </h1>
        <div id="pointsDisappear" class="text">
          <img id="dataDiagram" class="hidden" src="img/Diagram1_v1.png"/>
          <p>The massive Large Language Models (LLMs) of today could not exist without the internet proliferating data exponentially over the past decade. GPT-3 is a foundational language model trained on 45TB of text; downstream fine-tuned models are fluently searching, chatting, and coding. Despite its eerie capability, GPT-3 is only trained on a small subset of language itself. 85% of its training set was comprised of web text, including the Common Crawl, a dataset of scraped internet detritus.</p>
          <p id="data1">The much broader set of potential data remains inaccessible due to privitization, incompatible formats, or lack of digitization and aggregation. While current foundation models are language-based, nothing limits these models to dealing with text alone. The true potential of these models rests in their ability to synthesize multi-modal streams of information into a single knowledge architecture.</p>

          <p>This proposal seeks to radically expand the scope of foundation models, moving beyond anthropocentric, language-oriented data towards the wealth of non-anthropocentric information immanent to the planet. Should there be any hope that these foundation models can play a meaningful role in the prevention of climate collapse and ecological catastrophe, we must synthesize biosphere and technosphere.</p>

          <p id="data2">Many actors are already attempting to gather such information, albeit in piecemeal ways. The Global Nucleic Acid Observatory and the Australian Acoustic Observatory seek to capture metagenomic information of watersheds and ecological bioacoustic sound data, respectively. Both approaches invert the traditional model of the observatory, looking inwards towards the Earth rather than outwards into the cosmos. Still, they remain limited in scope and do not integrate the data they collect into a broader composition.</p>

          <p id="dataDisappear">Another structure provides a template for thinking through how a planetary-scale observatory might be conceived. The Event Horizon Telescope is a global network of synchronized radio telescopes that coordinated observations to image a black hole for the first time. Such a model recasts the entire earth as a distributed observatory, with each individual telescope contributing to a single composite image of the cosmos. Imagine a mesh observatory that instead takes the earth itself as the object to be observed. What if the cognitive infrastructure of such observations took the form of a foundation model, one which represented not only a small subset of human language, but the wealth of information in the biosphere as well?</p>
        </div>
      </div>
      <div id="act2" class="textSection">
        <h1>
          Act 2:<br />The Whole<br />Earth Codec
        </h1>
        <div class="text">
          <p>The Event Horizon Telescope produced the first image of a black hole by stitching together distributed measurements to output an inferred reconstruction. This calculation is no different to how we see: photoreceptors absorb electromagnetic radiation and trigger electrical signals traveling along neurons, beyond which higher levels of processing assemble the complete image. The transformation from one signal to another is called transduction, which describes both the processes by which we see, hear, smell—but also what machine sensors do.
          </p>
          <div id="transduction">
            <img src="img/Diagram4_v1.png"/>
          </div>
          <p>
          We are all plugged into a limited bandwidth of a planetary ground truth of various forms of radiation, vibration, and energy, yet integrate the stimuli into a cohesive umwelt. The biosphere and technosphere must be synchronized; enter the Whole Earth Codec, an autoregressive foundation model trained across myriad planetary sensing modalities. It enables interoperability between disparate forms of data and allows an expansive planetary intelligence to emerge.
          </p>
        <div id="codec" class="sticky visible">
          <img src="img/Diagram5_v1.png"/>
        </div>
        <h2 id="codecDisappear">Data</h2>
        <p>
        The Codec ingests broad spectrum data across modalities and is thus capable of synthesizing a wider understanding of the planet than any singular organism. The distributed network of the mesh observatory consists of different sensors receiving different types of stimuli: image, audio, chemical, lidar, pressure, moisture, magnetic fields, etc. What forms of data are produced are just as broad as the forms of sensing. The data stream from an individual sensor consists of measurements taken at a modality-relevant sampling rate, e.g. twenty times a second for earthquake seismometers, once every thirty minutes for common AQI sensors. The rate can also differ between sensors within the same modality, such as a microphone attuned to birdsong at 44 kHz versus one for turtles at 24 kHz. Regardless of modality, however, a UTC timestamp and GPS satellite signal is attached to each sample. This anchoring allows the model to make associations based on temporal and spatial correlation across disparate modes.
        </p>
        <h2>Pre-training</h2>
        <p>Foundation models are pre-trained on a massive corpus of unsupervised data, and the Whole Earth Codec is no different. To handle multimodal input, separate encoders are trained for each type of data. These encoders transform disparate forms of input into dense, high-dimensional embeddings within a single, massive cross-modal latent space. In order to do this, the model is trained to project temporally and spatially correlated forms of data into nearby embeddings within the space. Decoders of different modalities are then trained by translating the latent space embeddings into sequence predictions. Due to the massive size of data available, the model only makes a single pass over the continuous streams of data. This also means that as new data is gathered and aggregated over time, the model can simply continue training and updating weights.</p>
        <h2>Privacy</h2>
        <p>The distributed sensing network from which the Whole Earth Codec observes the planet is privy to vast amounts of data, yet sensitive information is protected through structured transparency techniques. Input privacy refers to the ability to process information that is hidden from you and to allow others to process your information without revealing it to them, while output privacy refers to the ability to read the output of an information flow without being able to reverse-engineer further information about the input. Through federated learning, data from the mesh observatory is processed in local servers within a secure enclave, communicating weights rather than data to the coordinating server containing the main foundation model. This maintains the input privacy of all data ingested by the model. For particularly sensitive types of data, adding noise to every datapoint preserves output privacy without adverse impacts on overall learned predictions; this technique is known as differential privacy. The sum of privacy-enhacing technologies deployed entails a trustless paradigm for training the Whole Earth Codec. Traditional relations of opacity/transparency and antagonism/mutualism are complicated by mutually assured observation.</p>
        <h2>Capabilities</h2>
        <p>Through ingesting myriad channels of sensing data and integrating them into a shared latent space, the Whole Earth Codec can make emergent associations between a multiplicity of temporal and spatial scales. Unmoored from the umwelt of any single organism, its sensorium is privy to an amalgamated landscape of previously indiscernable relations. Through the same abrupt specific capability scaling prevalent in LLMs, the foundation model sees sharp increases in task performance as the size of the training corpus increases; this motivates the Whole Earth Codec as a planetary project rather than a nation-state one.</p>
        <p>The Codec forms the substrate for a rich ecosystem of third-party, fine-tuned models with improved performance on downstream tasks. Leveraging the pre-trained baseline, fine-tuning involves using a smaller, labeled dataset to update model weights, often for specific capabilities or to address domain shift. Within the ecosystem, there are fine-tuned models developed by an economy of research universities and private startups, available open-source or through pay-to-play APIs. Openly available models proliferate in everyday use among climate-minded hobbyists, but industries such as insurance will pay a premium for high-performance proprietary software.</p>
        <h2>Governance</h2>
        <p>Sovereignty is derived from this technology, but its implementation spreads from the bottom up; the Whole Earth Codec cannot be owned by any one entity. The mesh observatory is a conglomerate of public and private sensors, networked in by organizations ranging from government institutions to research universities to individual landowners. Much like the Internet Engineering Task Force (or IETF), the standards and protocols of the Whole Earth Codec is maintained by a supra-national body and proposed, developed, and reviewed in an open process. This body maintains interoperable protocols for training and deploying the foundation model, governing processes including data transmission, federation, and weight aggregation. </p>
        </div>
      </div>
      <div id="act3" class="textSection">
        <h1 id="caseDisappear">
          Act 3:<br />New Planet<br />Sensorium
        </h1>
        <div id="clouds">
          <video autoplay muted loop>
            <source src="https://users.dma.ucla.edu/~dalena/WEC/clouds.webm" type="video/mp4">
          </video></div>
        <div class="text">
          <div id="caseIntro">
            <p>
            One of the key features of foundation models is emergence; the self-supervised model and extremely large scale of the training data makes it hard to predict exactly what the downstream capabilities may appear. This emergent nature challenges the traditional observatory model, in which scientists conduct observations in order to test specific hypotheses. What is an observatory when the object of observation cannot be known in advance? Through the assemblage of the Whole Earth Codec, secondary and tertiary modes of observation might emerge: a metagenomic sequence might tell one about land use patterns, for example. Observations are assembled rather than conducted. This final section consists of three speculative case studies. Speculation is crucial to understanding the potential benefits and risks that the Codec poses to the world, and how it may unseat assumptions of sovereignty, surveillance, and global coordination.
            </p>
          </div>
          <div id="finetuned">
            <img id="finetunedDiagram" class="hidden" src="img/Diagram6_v1.png"/>
          </div>
          <div id="finetuned1" class="case">
              <h2>Cloud<br />Seeding</h2>
            <p>
              Households in Mongolia are dependent on burning unrefined coal for heating, and thus the country has some of the worst air quality in the world. To remain under internationally mandated pollution levels, the Ministry of Environment and Tourism experiments with cloud seeding in an effort to produce rain to clear out air particles. The strategy is effective, but citizens in northern China catch wind of the program and accuse Mongolia of “stealing rainfall.”
            </p><p>
              Chinese researchers develops an extremely effective fine-tuned model from the Codec for detecting cloud seeding, trained on object detection within satellite imagery and chemical analysis of air. The model is open-sourced and picked up by a team of university researchers in Somalia. While playing around with the model, they discover that cloud seeding is also occuring in drought-stricken farmlands by rogue agricultural companies looking to increase crop yield. This leads to a proliferation of sensors in the region and further development of fine-tuned models suited todetecting weather modification.
          </div>
          <div id="finetuned2" class="case">
            <h2>Forest<br />Blight</h2>
            <div id="forest">
              <video autoplay muted loop>
                <source src="https://users.dma.ucla.edu/~dalena/WEC/forest.webm" type="video/mp4">
              </video>
            </div>
            <p>
              Canada has been plagued by pine beetles and their symbiotic fungi. This has an adverse impact on the country’s timber industry, as the infestation decimates monocultural forests before they reach maturity. In order to mitigate risk, insurance companies implement a proprietary fine-tuned model in order to better predict the spread of the beetle. This information is kept from landowners, who see large hikes in their insurance premiums in areas where the beetle is predicted to spread. Insurance companies monopolize risk tolerance and are reluctant to prevent the spread of the beetle as they turn a profit from corresponding premium increases.
            </p>
            <p>
              Landowners learn that acoustic monitoring data, combined with other site-specific chemical measurements, can more accurately predict the spread of the beetle. Pooling their individual data, they fine-tune the Codec to produce a better model for beetle population prediction, forming a decentralized data union. Information, rather than labor power, becomes the leverage point. High-resolution predictions enable targeted eradication of the beetle infestation, improving forest health. To encourage participation in the network, differential access to the fine-tuned model is granted based on the degree of participation in the scheme. One must install sensors to increase the fidelity of the network in order to access it.
            </p>
          </div>
          <div id="finetuned3" class="case">
            <h2>Gene<br />Drive</h2>
            <p>
              CRISPR-Cas9 technologies have made the genetic engineering of ecosystems a possibility. Through a technique known as a gene drive, a particular suite of genes can be propagated through a population, permanently changing the genome of an entire species. There is an international moratorium on gene drives, but it is notoriously difficult to regulate. To combat this, the European Union uses the Codec to implement a large-scale nucleic acid observatory. The observatory makes use of a mesh network of water sensors, which conducts metagenomic sequencing of the surrounding watershed. Federated learning protects the sensitive genomic information from being compromised, as edge devices share only learned weights to the fine-tuned model.
            </p><p>
              In the United Kingdom, there is popular resistance to what is perceived as a mass surveillance system. Politicians reject the fine-tuned model in favor of a homegrown system based on nationwide data. In the EU, the program proves remarkably efficient in preventing new gene drives. The baseline metagenome of non-modified species is incorporated into the model and any modifications to this baseline are immediately detected. Meanwhile, the UK model lacks the scale necessary for its performance to meet the same benchmark, leaving the country vulnerable to gene drives.
            </p>
            <p>
              An ecoterrorist group exploits this vulnerability by engineering a gene drive that modifies beef cattle in order to render their meat tough and inedible, without harming the animals themselves. This strategy proves remarkably effective in reducing meat consumption and related emissions in the UK. Despite this, modified livestock are killed, farmers are bailed out, and the UK concedes to participation in the Whole Earth Codec’s nucleic acid observatory to prevent similar events from happening again.
            </p>
          </div>
        </div>
      </div>
        <div id="conclusion" class="textSection">
          <div id="finetuned4" class="text">
            <h2>Conclusion</h2>
            <p>
              As James C. Scott explains in Seeing Like a State, what can be seen by a state, platform, or protocol also determines what can be managed, pillaged, or profited from. While the Whole Earth Codec has been framed as a new model of the observatory, it would be remiss to think its observation passive. All three case studies exhibit a recursive dynamic: the act of observation re-discloses the biosphere as an informatic landscape, a technogeography wherein the biosphere and technosphere become productive of one another. Pine beetle management necessitates a densification of the sensing apparatus, which in turn reveals the forest as a technical object to be managed. This recursion characterizes all sensor networks.
</p><p>
The Whole Earth Codec complicates this dynamic by introducing an appendage for processing, integration, and newfound synthesis. Capable of universal transfer amongst data streams, the codec trades cybernetic loops for n-dimensional knots. The self-supervised, emergent nature of the foundation model complicates what it means to see, as one can never be entirely sure what lies downstream. The future trajectory of foundation models remains opaque, but if they truly represent the paradigm shift that their proponents claim, it is of paramount importance that such foundations be as representative as possible of the entire spectrum of information the planet produces about itself. Guided by planetary-scale sensing rather than myopic anthropocentrism, the Whole Earth Codec opens up a future of ambivalent possibility within high-resolution prediction and newly interoperable intelligence.
            </p>
          </div>
          <div id="water">
            <video autoplay muted loop>
              <source src="https://users.dma.ucla.edu/~dalena/WEC/water.webm" type="video/mp4">
            </video>
          </div>
        </div>

  </main>
</body>
<script src="js/scroll.js"></script>
</html>
