<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Whole Earth Codec</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="banner">
      <img id="bannerTitle" src="titles/1line-white.svg" />
    </div>
  </header>
  <main>
    <div id="allSections">
      <div id="titleSection" class="visible">
        <video id="points" autoplay muted loop>
          <source src="https://media.githubusercontent.com/media/Antikythera-xyz/whole-earth-codec/main/videos/dissolve.webm" type="video/mp4">
        </video>
        <img src="titles/2line-white.svg"/>
      </div>
      <div class="textSection" id="pointsDisappear">
        <p>
          Traditional models of the observatory have focused on gazing outward, towards the cosmos. The recent proliferation of planetary sensor networks has inverted this gaze, forming a new kind of planetary observatory that takes the earth itself as its object. Could we cast the entire earth as a distributed observatory, using a foundation model to compose a singular, synthetic representation of the planet? The current generation of models primarily deal with human language, their training corpus scraped from the detritus of the internet. We must widen the aperture of what these models observe to include the nonnhuman.
        </p>
        <p>
          The Whole Earth Codec is an autoregressive, multi-modal foundation model that allows the planet to observe itself. This proposal radically expands the scope of foundation models, moving beyond anthropocentric language data towards the wealth of ecological information immanent to the planet. Moving from raw sense data to high-dimensional embedding in latent space, the observatory folds in on itself, thus revealing a form of computational reason that transcends sense perception alone: a sight beyond sight. Guided by planetary-scale sensing rather than myopic anthropocentrism, the Whole Earth Codec opens up a future of ambivalent possibility through cross-modal meta-observation, perhaps generating a form of planetary sapience.
        </p>
        <p>
          <a href="./WholeEarthCodec_290623.pdf" target="blank">Read →</a>
        </p>
      </div>
      <div id="diagramSection">
        <div class="centerimg">
          <img id="diagram" src="img/diagram_transparent.png" />
        </div>
        <div id="sensing" class="labels hlabel">Sensing Layer</div>
        <div id="fm" class="labels hlabel">Foundation Model</div>
        <div id="sensors" class="labels slabel">Sensors</div>
        <div id="federation" class="labels slabel">Federated Learning</div>
        <div id="encoders" class="labels slabel">Encoders</div>
        <div id="latent" class="labels slabel">Latent Space</div>
        <div id="finetuned" class="labels slabel">Fine-Tuned Models</div>
        <div id="decoders" class="labels slabel">Decoders</div>
        <div id="description" class="labels">
          The sensing layer is where the multi-modal data of the biosphere is transduced, recorded, and digitized. Its topology is a distributed mesh network containing federated edge devices and regional data centers. Each edge device might consist of different sensors receiving different types of stimuli: image, audio, chemical, lidar, pressure, moisture, magnetic fields.
        </div>
      </div>
      <div id="wecVid">
        <iframe src="https://player.vimeo.com/video/840565197?badge=0&autopause=0&player_id=0&app_id=58479/embed" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
      </div>
      <div id="creditSection">
        <div id="credits">
          <div class="box">
            <b>Studio Researchers</b><br />
            Connor Cook<br />
            Christina Lu<br />
            Dalena Tran<br />
          </div>
          <div class="box">
            <b>Program Director</b><br />
            Benjamin Bratton<br /><br />
            <b>Studio Director</b><br />
            Nicolay Boyadjiev<br /><br />
            <b>Associate Director</b><br />
            Stephanie Sherman<br />
          </div>
          <div class="box">
            <b>Senior Program Manager</b><br />
            Emily Knapp<br /><br />
            <b>Network Operatives</b><br />
            Dasha Silkina<br />
            Andrew Karabanov<br /><br />
            <b>Art Direction</b><br />
            Case Miller<br /><br />
          </div>
          <div class="box">
            <b>Sound Design</b><br />
            Błażej Kotowski<br /><br />
            <b>Graphic Design</b><br />
            Callum Dean<br /><br />
            <b>Voiceover Engineer</b><br />
            Sam Horn<br /><br />
            <b>Editor</b><br />
            Guy Mackinnon-Little<br />
          </div>
          <div class="box">Thanks to The Berggruen Institute and One Project for their support for the inaugural year of Antikythera.
            <br /><br />
              Special thanks to Nicolas Berggruen, Nils Gilman, Dawn Nakagawa, Justin Rosenstein, and Raphael Arar for their visionary support and participation.
              <br />
              <a href="https://antikythera.org"><img id="logo" src="img/antikythera.svg" /></a>
          </div>
        </div>
        <p class="email">
          <span>Press and inquiries → whole.earth.codec</span><span style="display:none;">no scraping</span><span>@</span><span>gmail.com</span>
        </p>
      </div>
    </div>
  </main>
</body>
<script src="js/scroll.js"></script>
</html>
