<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>WHOLE EARTH CODEC</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="banner">
      <div id="title">WHOLE EARTH CODEC</div>
    </div>
  </header>
  <main>
      <div id="titleSection">
        <div class="titleText">WHOLE<br />EARTH<br />CODEC
        </div>
      </div>
      <div id="toc">
        <div class="contents">
          <h1>
            Act 1: Mesh Observatory
          </h1>
          <p>
            Inverting the gaze, synthesizing data streams.
          </p>
        </div>
        <div class="contents">
          <h1>
            Act 2: Biosphere and Technosphere
          </h1>
          <p>
            Digitizing Earth’s Umwelt.
          </p>
        </div>
        <div class="contents">
          <h1>
            Act 3: New Planet Sensorium
          </h1>
          <p>
            Amalgamated and augmented landscape.
          </p>
        </div>
      </div>
      <div class="textSection">
        <h1>
          Act 1:<br />Mesh<br />Observatory
        </h1>
        <div class="text">
          <p>In 2022, AI entered the cultural consciousness like never before. User-friendly applications such as ChatGPT and Dall-E Mini invited participation across a broad cross-section of society. This widespread popularity is due in large part to a large improvement in the performance and fidelity of the system, thanks to a paradigmatic shift in the underlying model architecture. These recent models are known as foundation models: large general purpose models trained on broad data, using self-supervised methods. Self-supervised deep neural networks are nothing new, but the sheer scale of the data involved set these models apart from their predecessors. As their name suggests, foundation models can serve as the pre-trained basis for a wide range of downstream tasks. Fine-tuned models can repurpose the original model for tasks that the original model was never designed to perform. This makes them both highly useful and consequent: a shaky foundation is prone to collapse down the line. This makes the crafting of representative, rich, and diverse foundation models of paramount importance.
          </p>
          <div id="behind">
            <img src="img/FunnelMockup_transparent.png"/>
          </div>
          <p>While foundation models have the potential to be trained on many modalities of data, to date most development has taken place in the realm of natural language processing. OpenAI’s ChatGPT is what is known as a large language model. Despite their impressive scale (the underlying model, GPT-3, has 175 billion parameters) these models only represent a very small subset of data. The model is trained primarily using web data scraped from Common Crawl, a small and unrepresentative data set representing the “detritus” of human speech. Much of the potential data remains inaccessible due to privacy concerns, incompatible format, or lack of proper aggregation. [dalenaxtran@gmail.com insert something about data lakes]. If we are to construct large models that serve as the foundation for a wide range of future tasks, we must strive to overcome this technical challenge and to include as much data as possible.
        </p>
        </div>
      </div>
      <div class="textSection">
        <h1>
          Act 2:<br />Biosphere and<br />Technosphere
        </h1>
        <div class="text">
          <p>In 2022, AI entered the cultural consciousness like never before. User-friendly applications such as ChatGPT and Dall-E Mini invited participation across a broad cross-section of society. This widespread popularity is due in large part to a large improvement in the performance and fidelity of the system, thanks to a paradigmatic shift in the underlying model architecture. These recent models are known as foundation models: large general purpose models trained on broad data, using self-supervised methods. Self-supervised deep neural networks are nothing new, but the sheer scale of the data involved set these models apart from their predecessors. As their name suggests, foundation models can serve as the pre-trained basis for a wide range of downstream tasks. Fine-tuned models can repurpose the original model for tasks that the original model was never designed to perform. This makes them both highly useful and consequent: a shaky foundation is prone to collapse down the line. This makes the crafting of representative, rich, and diverse foundation models of paramount importance.
          </p>
          <div id="behind">
            <img src="img/FunnelMockup_transparent.png"/>
          </div>
          <p>While foundation models have the potential to be trained on many modalities of data, to date most development has taken place in the realm of natural language processing. OpenAI’s ChatGPT is what is known as a large language model. Despite their impressive scale (the underlying model, GPT-3, has 175 billion parameters) these models only represent a very small subset of data. The model is trained primarily using web data scraped from Common Crawl, a small and unrepresentative data set representing the “detritus” of human speech. Much of the potential data remains inaccessible due to privacy concerns, incompatible format, or lack of proper aggregation. [dalenaxtran@gmail.com insert something about data lakes]. If we are to construct large models that serve as the foundation for a wide range of future tasks, we must strive to overcome this technical challenge and to include as much data as possible.
        </p>
        </div>
      </div>
  </main>
</body>
<script src="js/headerScroll.js"></script>
</html>
